<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Conversational AI - LangChain</title>
  <link rel="stylesheet" href="../styles/main.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
  <header>
    <nav>
      <h1>Tech Overview</h1>
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="../index.html#technologies">Technologies</a></li>
      </ul>
    </nav>
  </header>

  <div class="breadcrumb">
    <a href="../index.html">Home</a> / <a href="langchain.html">LangChain</a> / <span>Conversational AI</span>
  </div>

  <main>
    <a href="langchain.html" class="back-link">← Back to LangChain</a>

    <section class="use-case-detail">
      <h2>Conversational AI with LangChain</h2>
      <p>Building intelligent chatbots and assistants with multi-turn conversations, memory management, and context-aware responses.</p>

      <h3>Overview</h3>
      <p>LangChain's memory and chain abstractions enable sophisticated conversational experiences. Support for different memory types ensures flexible conversation management across sessions.</p>

      <h3>Basic Chatbot with Memory</h3>
      <div class="code-label">Python - Simple Conversational Chain</div>
      <pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.prompts import PromptTemplate

# Initialize LLM
llm = ChatOpenAI(model="gpt-4")

# Create memory to store conversation history
memory = ConversationBufferMemory()

# Create conversation chain
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# Multi-turn conversation
response1 = conversation.predict(input="Hi, I'm looking for help with Python")
print(response1)

response2 = conversation.predict(input="Can you explain decorators?")
print(response2)

response3 = conversation.predict(input="How would I use them?")
print(response3)  # LLM remembers previous context</code></pre>

      <h3>Advanced Memory Types</h3>
      <div class="code-label">Python - Different Memory Strategies</div>
      <pre><code class="language-python">from langchain.memory import (
    ConversationBufferMemory,
    ConversationSummaryMemory,
    ConversationBufferWindowMemory,
    ConversationTokenBufferMemory,
)

# 1. Buffer Memory - Stores full conversation
buffer_memory = ConversationBufferMemory()

# 2. Summary Memory - Summarizes conversation over time
summary_memory = ConversationSummaryMemory(
    llm=llm,
    buffer="The user is learning Python programming",
)

# 3. Window Memory - Only keeps last k messages
window_memory = ConversationBufferWindowMemory(k=5)

# 4. Token Buffer Memory - Limits by token count
token_memory = ConversationTokenBufferMemory(
    llm=llm,
    max_token_limit=2000
)

# Use with conversation chain
for memory_type, memory_instance in [
    ("Buffer", buffer_memory),
    ("Summary", summary_memory),
    ("Window", window_memory),
]:
    conversation = ConversationChain(
        llm=llm,
        memory=memory_instance,
    )
    print(f"\n=== {memory_type} Memory ===")
    print(conversation.predict(input="What is machine learning?"))</code></pre>

      <h3>Chatbot with Custom Personality</h3>
      <div class="code-label">Python - Personality-Driven Assistant</div>
      <pre><code class="language-python">from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# Define system personality
system_template = """You are a helpful Python programming tutor.
Your style is:
- Friendly and encouraging
- Provide code examples when relevant
- Explain concepts clearly for beginners
- Ask follow-up questions if needed
- Use analogies to clarify complex topics

Current conversation history:
{history}"""

system_prompt = SystemMessagePromptTemplate.from_template(system_template)

human_template = "{input}"
human_prompt = HumanMessagePromptTemplate.from_template(human_template)

prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])

# Create memory and chain
memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm,
    prompt=prompt,
    memory=memory,
    verbose=True
)

# Interact with personalized assistant
print(conversation.predict(input="I'm confused about list comprehensions"))
print(conversation.predict(input="Can you show me an example?"))</code></pre>

      <h3>Multi-Agent Conversation</h3>
      <div class="code-label">Python - Multi-Turn Agent Dialogue</div>
      <pre><code class="language-python">from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.tools import tool
from langchain.memory import ConversationBufferMemory

# Define available tools
@tool
def search_documentation(query: str) -> str:
    """Search Python documentation"""
    return f"Documentation results for: {query}"

@tool
def execute_code(code: str) -> str:
    """Execute Python code and return output"""
    try:
        exec_globals = {}
        exec(code, exec_globals)
        return "Code executed successfully"
    except Exception as e:
        return f"Error: {str(e)}"

@tool
def get_tutorial(topic: str) -> str:
    """Get tutorial links for a topic"""
    return f"Tutorial links for {topic}: https://tutorial.example.com/{topic}"

# Create agent
tools = [search_documentation, execute_code, get_tutorial]
memory = ConversationBufferMemory(memory_key="chat_history")

agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt="You are a Python programming assistant..."
)

agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True
)

# Multi-turn conversation with tool use
result1 = agent_executor.invoke({
    "input": "Show me how to use list comprehensions"
})

result2 = agent_executor.invoke({
    "input": "Can you execute a simple example?"
})</code></pre>

      <h3>FAQ Chatbot</h3>
      <div class="code-label">Python - FAQ-Driven Assistant</div>
      <pre><code class="language-python">from langchain.chains import ConversationalRetrievalChain
from langchain.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS

# Load FAQ documents
loader = TextLoader("faq.txt")
documents = loader.load()

# Create vector store from FAQs
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)
splits = text_splitter.split_documents(documents)
vectorstore = FAISS.from_documents(splits, OpenAIEmbeddings())

# Create conversation retrieval chain
memory = ConversationBufferMemory(memory_key="chat_history")
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory,
)

# Chat with FAQ bot
print(qa_chain({"question": "What is your return policy?"}))
print(qa_chain({"question": "How long does shipping take?"}))
print(qa_chain({"question": "When did I ask about return policy?"}))  # Uses memory</code></pre>

      <h3>Emotional Intelligence</h3>
      <div class="code-label">Python - Emotion-Aware Chatbot</div>
      <pre><code class="language-python">from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Emotion detection
emotion_template = """Analyze the emotional tone of this message and respond appropriately.

Message: {user_message}

Detected emotion: [Analyze and respond with empathy]
Response:"""

emotion_prompt = PromptTemplate(
    template=emotion_template,
    input_variables=["user_message"]
)

emotion_chain = LLMChain(llm=llm, prompt=emotion_prompt)

# Conversation with emotional awareness
messages = [
    "I'm really frustrated with this bug",
    "I finally got it working!",
    "This seems impossible",
]

for msg in messages:
    response = emotion_chain.run(user_message=msg)
    print(f"User: {msg}")
    print(f"Bot: {response}\n")</code></pre>

      <h3>Session Persistence</h3>
      <div class="code-label">Python - Persistent Conversations</div>
      <pre><code class="language-python">import json
from datetime import datetime
from pathlib import Path

class PersistentChatbot:
    def __init__(self, session_id: str, llm):
        self.session_id = session_id
        self.llm = llm
        self.session_file = Path(f"sessions/{session_id}.json")
        self.memory = ConversationBufferMemory()
        self.load_session()
        
        self.conversation = ConversationChain(
            llm=llm,
            memory=self.memory,
        )
    
    def load_session(self):
        """Load previous conversation history"""
        if self.session_file.exists():
            with open(self.session_file) as f:
                data = json.load(f)
                for msg in data.get("messages", []):
                    if msg["role"] == "user":
                        self.memory.buffer += f"Human: {msg['content']}\n"
                    else:
                        self.memory.buffer += f"AI: {msg['content']}\n"
    
    def save_session(self):
        """Save conversation to disk"""
        self.session_file.parent.mkdir(exist_ok=True)
        messages = self._parse_messages()
        with open(self.session_file, "w") as f:
            json.dump({
                "session_id": self.session_id,
                "created": datetime.now().isoformat(),
                "messages": messages
            }, f)
    
    def _parse_messages(self) -> list:
        """Extract messages from memory buffer"""
        # Implementation to parse memory buffer into message list
        pass
    
    def chat(self, user_input: str) -> str:
        """Process user message and save"""
        response = self.conversation.predict(input=user_input)
        self.save_session()
        return response

# Usage
bot = PersistentChatbot("user_123", llm)
response = bot.chat("Hi, remember what we discussed yesterday?")
print(response)</code></pre>

      <h3>Best Practices</h3>
      <ul>
        <li>Choose appropriate memory type based on conversation length</li>
        <li>Use summary memory for long conversations to save tokens</li>
        <li>Include system prompts to define personality and behavior</li>
        <li>Implement rate limiting and message validation</li>
        <li>Persist conversations for continuity across sessions</li>
        <li>Monitor token usage to control costs</li>
        <li>Add error handling for API failures</li>
      </ul>
    </section>
  </main>

  <footer class="footer">
    <p>Technology Overview • January 2026</p>
  </footer>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-kotlin.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
</body>
</html>