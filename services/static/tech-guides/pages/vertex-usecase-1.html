<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Computer Vision - GCP Vertex AI</title>
  <link rel="stylesheet" href="../styles/main.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
  <header>
    <nav>
      <h1>Tech Overview</h1>
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="../index.html#technologies">Technologies</a></li>
      </ul>
    </nav>
  </header>

  <div class="breadcrumb">
    <a href="../index.html">Home</a> / <a href="vertex-ai.html">GCP Vertex AI</a> / <span>Computer Vision</span>
  </div>

  <main>
    <a href="vertex-ai.html" class="back-link">← Back to GCP Vertex AI</a>

    <section class="use-case-detail">
      <h2>Computer Vision with Vertex AI</h2>
      <p>Building vision models for image classification, object detection, and custom visual recognition at enterprise scale with minimal ML expertise.</p>

      <h3>Overview</h3>
      <p>Vertex Vision AutoML enables quick model development without custom training code. For advanced needs, use custom training with TensorFlow or PyTorch on Vertex Training infrastructure.</p>

      <h3>Quick Start: AutoML Image Classification</h3>
      <div class="code-label">Python - AutoML Setup</div>
      <pre><code class="language-python">from google.cloud import aiplatform
import json

# Initialize Vertex AI
aiplatform.init(project="your-project-id", location="us-central1")

# Create dataset
dataset = aiplatform.ImageDataset.create(
    display_name="product-images",
    gcs_source="gs://your-bucket/images/train.csv",  # CSV with img_path, label
)

# Training pipeline for image classification
job = aiplatform.AutoMLImageTrainingJob(
    display_name="product-classifier-v1",
    prediction_type="classification",
    multi_label=False,  # Single-label classification
    model_type="CLOUD",  # Cloud-hosted model
)

model = job.run(
    dataset=dataset,
    budget_milli_node_hours=8000,  # Training budget
    disable_early_stopping=False,
)</code></pre>

      <h3>Custom Training Pipeline</h3>
      <div class="code-label">Python - Custom Training</div>
      <pre><code class="language-python">from google.cloud import aiplatform
import tensorflow as tf
from tensorflow.keras import layers, models

# Define custom model
def create_model():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    return model

# Training function (saved as train.py)
def train_and_save():
    # Load data
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    
    # Normalize
    x_train, x_test = x_train / 255.0, x_test / 255.0
    
    # Build and compile
    model = create_model()
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # Train
    model.fit(
        x_train, y_train,
        epochs=10,
        batch_size=32,
        validation_data=(x_test, y_test)
    )
    
    # Save to GCS
    model.save('gs://your-bucket/models/cifar10')

# Create custom training job
aiplatform.init(project="your-project-id")

job = aiplatform.CustomTrainingJob(
    display_name="custom-image-classifier",
    script_path="train.py",  # Your training script
    container_uri="gcr.io/cloud-aiplatform/training/tf-cpu.2-12",
    requirements=["tensorflow==2.12"],
)

model = job.run(
    replica_count=1,
    machine_type="n1-standard-4",
)</code></pre>

      <h3>Object Detection Model</h3>
      <div class="code-label">Python - Object Detection Training</div>
      <pre><code class="language-python">from google.cloud import aiplatform

# Create object detection dataset
dataset = aiplatform.ImageDataset.create(
    display_name="product-detection",
    gcs_source="gs://your-bucket/detection/annotations.csv",
    import_schema_uri=aiplatform.schema.dataset.ioformat.image_bounding_box.single_label_classification,
)

# Training job for object detection
job = aiplatform.AutoMLImageTrainingJob(
    display_name="product-detector",
    prediction_type="object_detection",
    model_type="CLOUD",
)

model = job.run(
    dataset=dataset,
    budget_milli_node_hours=20000,  # More budget for detection
)</code></pre>

      <h3>Batch Prediction</h3>
      <div class="code-label">Python - Large-Scale Inference</div>
      <pre><code class="language-python">from google.cloud import aiplatform

# Get deployed model
model = aiplatform.Model("your-model-resource-id")

# Create batch prediction job
batch_job = model.batch_predict(
    job_display_name="product-images-batch",
    gcs_source="gs://your-bucket/images/predict/*.jpg",
    gcs_destination_prefix="gs://your-bucket/predictions/",
)

# Wait for completion
batch_job.wait()

# Process results
print(f"Output location: {batch_job.output_info.gcs_output_directory}")</code></pre>

      <h3>Real-Time Predictions</h3>
      <div class="code-label">Python - Endpoint Deployment</div>
      <pre><code class="language-python">from google.cloud import aiplatform
import base64

# Deploy model to endpoint
endpoint = model.deploy(
    machine_type="n1-standard-4",
    accelerator_type="NVIDIA_TESLA_K80",
    accelerator_count=1,
    traffic_percentage=100,
)

# Make predictions
import json
from PIL import Image
from io import BytesIO

# Load image
with open("product.jpg", "rb") as img_file:
    image_base64 = base64.b64encode(img_file.read()).decode()

# Prepare prediction request
instances = [{
    "data": image_base64
}]

predictions = endpoint.predict(instances=instances)

for pred in predictions:
    print(f"Top classes: {pred['displayNames'][:3]}")
    print(f"Confidences: {pred['confidences'][:3]}")</code></pre>

      <h3>Model Evaluation & Monitoring</h3>
      <div class="code-label">Python - Model Performance Tracking</div>
      <pre><code class="language-python">from google.cloud import aiplatform
from sklearn.metrics import classification_report, confusion_matrix

# Get model evaluation
model = aiplatform.Model("your-model-resource-id")
model_eval = model.get_model_evaluation()

print("Model Evaluation Metrics:")
print(f"Accuracy: {model_eval.metrics['accuracy']}")
print(f"Precision: {model_eval.metrics['precision']}")
print(f"Recall: {model_eval.metrics['recall']}")

# Setup monitoring
job = aiplatform.ModelMonitoringJob.create(
    display_name="model-monitoring",
    objective_config=aiplatform.ModelMonitoringObjectiveConfig(
        skew_detection_config=aiplatform.ModelMonitoringAlertConfig(
            data_drift_threshold=0.1,
            attribution_score_skew_threshold=0.05,
        ),
        prediction_drift_config=aiplatform.ModelMonitoringAlertConfig(
            data_drift_threshold=0.15,
        ),
    ),
)</code></pre>

      <h3>Advanced: Custom Model with TensorFlow</h3>
      <div class="code-label">Python - Production Model Code</div>
      <pre><code class="language-python">import tensorflow as tf
from google.cloud import storage
import os

class ProductClassifier(tf.keras.Model):
    def __init__(self, num_classes):
        super().__init__()
        self.base = tf.keras.applications.EfficientNetB0(
            input_shape=(224, 224, 3),
            include_top=False,
            weights='imagenet'
        )
        self.global_avg = tf.keras.layers.GlobalAveragePooling2D()
        self.dropout = tf.keras.layers.Dropout(0.2)
        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax')
    
    def call(self, inputs, training=None):
        x = self.base(inputs, training=False)
        x = self.global_avg(x)
        x = self.dropout(x, training=training)
        return self.dense(x)

# Training wrapper for Vertex
def train_and_export(args):
    # Load data
    train_dataset = tf.data.Dataset.from_tensor_slices(...)
    train_dataset = train_dataset.shuffle(1000).batch(32)
    
    # Create model
    model = ProductClassifier(num_classes=10)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # Train
    model.fit(train_dataset, epochs=10)
    
    # Save to GCS
    bucket = storage.Client().bucket(args.bucket)
    model.save('/tmp/model')
    for file in os.listdir('/tmp/model'):
        blob = bucket.blob(f"models/{args.model_name}/{file}")
        blob.upload_from_filename(f"/tmp/model/{file}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--bucket")
    parser.add_argument("--model-name")
    args = parser.parse_args()
    train_and_export(args)</code></pre>

      <h3>Best Practices</h3>
      <ul>
        <li>Start with AutoML for rapid prototyping</li>
        <li>Use balanced datasets to avoid bias</li>
        <li>Implement data validation before training</li>
        <li>Monitor model performance in production</li>
        <li>Set up retraining pipelines for data drift</li>
        <li>Use batch prediction for large-scale inference</li>
        <li>Implement proper access controls and versioning</li>
      </ul>
    </section>
  </main>

  <footer class="footer">
    <p>Technology Overview • January 2026</p>
  </footer>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-kotlin.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
</body>
</html>